{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Makemore MLP with batch normalisation",
   "id": "a332b953cf9f5788"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports + dataset construction",
   "id": "88e4483a20b2b4b8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import bisect\n",
    "\n",
    "\n",
    "words = open('names.txt').read().splitlines()\n",
    "\n",
    "# Utility dicts\n",
    "stoi = {c:i+1 for i,c in enumerate(sorted(set(\"\".join(words))))}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i:c for c,i in stoi.items()}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create the dataset\n",
    "CONTEXT_WINDOW = 3\n",
    "def build_dataset(wds):\n",
    "    X,Y = [],[]\n",
    "    for w in wds:\n",
    "        window = deque([0]*CONTEXT_WINDOW,maxlen=CONTEXT_WINDOW)\n",
    "        for c in w + \".\":\n",
    "            ix = stoi[c]\n",
    "            X.append(list(window))\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in window), '--->', itos[ix])\n",
    "            window.append(ix)\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    print(X.shape,Y.shape)\n",
    "    return X,Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ],
   "id": "e48546aae9932108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model with BatchNorm - Manual implementation",
   "id": "71b9960cbc834829"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Keys to note**:\n",
    "\n",
    "- Batch Normalisation layer added in after the preactivation of the hidden layer to ensure unit gaussian preacts on initialisation.\n",
    "- BatchNorm layer has its own bngain and bnbias to be learned via backprop. This removes need for bias in hidden layer as it will be subtracted in the BatchNorm layer anyway and a separate bias will be added\n",
    "\n",
    "**Issue** -> Batch normalisation leads to coupling of examples, we use a different sample mean/sd per batch.\n",
    "\n",
    "This means for inference, we need to either calculate the overall sample mean/sd as an additional step,\n",
    "\n",
    "OR\n",
    "\n",
    "we can use a running mean/sd during training that will converge on the total sample mean/sd estimate"
   ],
   "id": "636ee30f07036ee1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Embedding Layer\n",
    "N = 10\n",
    "C = torch.randn((27,N))\n",
    "emb = C[Xtr]\n",
    "\n",
    "# Linear Layer (no bias needed)\n",
    "W1 = torch.randn((N*CONTEXT_WINDOW,300))\n",
    "# b1 = torch.randn(300) * 0.01\n",
    "embcat = emb.view(-1,N*CONTEXT_WINDOW)\n",
    "preact = embcat @ W1\n",
    "\n",
    "# BatchNorm layer\n",
    "bngain = torch.ones(300)\n",
    "bnbias = torch.zeros(300)\n",
    "# Initialise running mean/std\n",
    "bnmean_running = torch.zeros((1, 300))\n",
    "bnstd_running = torch.ones((1, 300))\n",
    "\n",
    "# Output layer\n",
    "\n",
    "W2 = torch.randn((300,27)) / 300 **0.5\n",
    "b2 = torch.randn(27) * 0\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ],
   "id": "9edf0f5217d55bfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "milestones = [50000, 75000, 100000]\n",
    "values = [0.1, 0.05, 0.03,0.01]\n",
    "losses = []\n",
    "steps = []\n",
    "for i in range(200000):\n",
    "    # Batch feature vectors\n",
    "    ix = torch.randint(0, Xtr.shape[0], (64,))\n",
    "    emb = C[Xtr[ix]]\n",
    "    embcat = emb.view(-1,N*CONTEXT_WINDOW)\n",
    "    # Linear layer\n",
    "    preact = embcat @ W1\n",
    "    # BatchNorm layer\n",
    "    bnmeani = preact.mean(0, keepdim=True)\n",
    "    bnstdi = preact.std(0, keepdim=True)\n",
    "    preact = bngain * (preact - bnmeani) / bnstdi + bnbias\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    # Nonlinearity\n",
    "    h = torch.tanh(preact)\n",
    "    # Output Layer\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = values[bisect.bisect_right(milestones, i)]\n",
    "    for p in parameters:\n",
    "        p.data -= p.grad * lr\n",
    "    steps.append(i)\n",
    "    losses.append(loss.item())\n",
    "    if i % 1000 == 0:\n",
    "        print(loss.item())\n"
   ],
   "id": "f764d7104e4e730a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Results",
   "id": "3477a31eb13f7e50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The loss achieved with the Batch Norm layer is comparable to that with just kaiming init scaling on initialisation",
   "id": "a84aaa7920a7c7b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "plt.plot(torch.tensor(losses).view(-1,1000).mean(1), \"g\" )\n",
    "plt.suptitle(\"Loss Fluctuation vs steps\", fontweight='bold')\n",
    "plt.title(\"Training on the training set\")\n",
    "plt.show()\n"
   ],
   "id": "fe094ab5e82371c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5ac83e9e1e2c0fe9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training loss",
   "id": "9fcff6578807c6a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emb = C[Xtr]\n",
    "preact = emb.view(-1,N*CONTEXT_WINDOW) @ W1\n",
    "preact = bngain * (preact - bnmean_running) / bnstd_running + bnbias\n",
    "h = torch.tanh(preact)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ],
   "id": "240284c3d8785ab1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test loss",
   "id": "61237675f0168ce9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emb = C[Xte]\n",
    "preact = emb.view(-1,N*CONTEXT_WINDOW) @ W1\n",
    "preact = bngain * (preact - bnmean_running) / bnstd_running + bnbias\n",
    "h = torch.tanh(preact)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Yte)\n",
    "loss"
   ],
   "id": "10c79ebc4fc7be16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that the result of the tanh neurons are not saturated, even if we remove the kaiming init factor from the initialisation of W1.",
   "id": "76b6fae26d921b09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.imshow((h.abs() > 0.99)[:32], cmap = 'gray' )",
   "id": "9c48aba31d29960c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For inference, we use the running mean/std calculated",
   "id": "1988b6a684cbb17e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for _ in range(100):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * CONTEXT_WINDOW\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]  # (1,block_size,d)\n",
    "        preact = emb.view(-1,N*CONTEXT_WINDOW) @ W1\n",
    "        preact = bngain * (preact - bnmean_running) / bnstd_running + bnbias\n",
    "        h = torch.tanh(preact)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ],
   "id": "7644ee1fa687fda2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model with BatchNorm - Pytorch style implementation with diagnostic plots",
   "id": "97dc45b1f4f5c725"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model",
   "id": "9a9a859ac3268bab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Layer classes like pytorch",
   "id": "e93ca43446c73fab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Linear:\n",
    "    def __init__(self, ni: int, no: int, bias: bool=True):\n",
    "        self.w = torch.randn((ni,no)) / ni **0.5\n",
    "        self.bias = torch.randn(no) * 0 if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.output = x @ self.w + (self.bias if self.bias is not None else 0)\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.w, self.bias] if self.bias is not None else [self.w]\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, ni: int, eps: float=1e-5, momentum: float=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.ni = ni\n",
    "        self.gamma = torch.ones(ni)\n",
    "        self.beta = torch.zeros(ni)\n",
    "        self.training = True\n",
    "        self.running_mean = torch.zeros(ni)\n",
    "        self.running_var = torch.ones(ni)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(dim=0, keepdim=True)\n",
    "            xvar = x.var(dim=0, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "\n",
    "        x_standard = (x - xmean) / (xvar + self.eps)**0.5\n",
    "        self.output = self.gamma * x_standard + self.beta\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1-self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.output\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.output = torch.tanh(x)\n",
    "        return self.output\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "N = 10\n",
    "VOCAB_SIZE = 27\n",
    "HIDDEN_SIZE = 100\n",
    "C = torch.randn((VOCAB_SIZE,N))\n",
    "\n",
    "# layers = [\n",
    "#     Linear(N*CONTEXT_WINDOW, HIDDEN_SIZE), Tanh(),\n",
    "#     Linear(HIDDEN_SIZE, HIDDEN_SIZE), Tanh(),\n",
    "#     Linear(HIDDEN_SIZE, HIDDEN_SIZE), Tanh(),\n",
    "#     Linear(HIDDEN_SIZE, HIDDEN_SIZE), Tanh(),\n",
    "#     Linear(HIDDEN_SIZE, VOCAB_SIZE),\n",
    "# ]\n",
    "\n",
    "layers = [\n",
    "    Linear(N*CONTEXT_WINDOW, HIDDEN_SIZE, bias = False), BatchNorm1d(HIDDEN_SIZE), Tanh(),\n",
    "    Linear(HIDDEN_SIZE, HIDDEN_SIZE, bias = False), BatchNorm1d(HIDDEN_SIZE), Tanh(),\n",
    "    Linear(HIDDEN_SIZE, HIDDEN_SIZE, bias = False), BatchNorm1d(HIDDEN_SIZE), Tanh(),\n",
    "    Linear(HIDDEN_SIZE, HIDDEN_SIZE, bias = False), BatchNorm1d(HIDDEN_SIZE), Tanh(),\n",
    "    Linear(HIDDEN_SIZE, HIDDEN_SIZE, bias = False)\n",
    "    ,\n",
    "]\n",
    "# layers = [\n",
    "#     Linear(N*CONTEXT_WINDOW, HIDDEN_SIZE),\n",
    "#     Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "#     Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "#     Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
    "#     Linear(HIDDEN_SIZE, VOCAB_SIZE),\n",
    "# ]\n",
    "\n",
    "with torch.no_grad():\n",
    "    layers[-1].w *= 0.1\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.w *= 5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n"
   ],
   "id": "c51da1c937914ff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr = 0.1\n",
    "losses = []\n",
    "updates = []\n",
    "\n",
    "for i in range(200000):\n",
    "    # Indices of the batch\n",
    "    ix = torch.randint(0,Xtr.shape[0],(64,))\n",
    "\n",
    "    # Get batch embeddings\n",
    "    emb = C[Xtr[ix]]\n",
    "    x = emb.view(-1,N*CONTEXT_WINDOW)\n",
    "    # Forward through all layers\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "\n",
    "    loss = F.cross_entropy(x, Ytr[ix])\n",
    "    # Backward\n",
    "    for layer in layers:\n",
    "        layer.output.retain_grad()\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in parameters:\n",
    "        p.data -= p.grad * lr\n",
    "\n",
    "    # Stat tracking\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"{i}: {loss.item():.3f}\")\n",
    "    losses.append(loss.item())\n",
    "    with torch.no_grad():\n",
    "        updates.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "    # break\n"
   ],
   "id": "e3856da15f5c28c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plots",
   "id": "3ce58166ce32333c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Losses of the deeper network:\n",
    "\n",
    "more overfitting as more params"
   ],
   "id": "49063061c74a2212"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(range(len(losses)), losses, \"g\" )\n",
    "plt.suptitle(\"Loss Fluctuation vs steps\", fontweight='bold')\n",
    "plt.title(\"Training on the training set\")\n",
    "plt.show()"
   ],
   "id": "2c43e865c7ee162f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# emb = C[Xtr]\n",
    "# x = emb.view(-1,N*CONTEXT_WINDOW)\n",
    "# for layer in layers:\n",
    "#     x = layer(x)\n",
    "# loss = F.cross_entropy(x, Ytr)\n",
    "# loss"
   ],
   "id": "b8d164724a606492",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# emb = C[Xte]\n",
    "# x = emb.view(-1,N*CONTEXT_WINDOW)\n",
    "# for layer in layers:\n",
    "#     x = layer(x)\n",
    "# loss = F.cross_entropy(x, Yte)\n",
    "# loss"
   ],
   "id": "c15796110d1758d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Activations of layers",
   "id": "d5f89d59ae290f07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notes:\n",
    "\n",
    "- If we don't add any gain, with just linear and tanh layers, the activations will squeeze in over time due to the tanh function decreasing the variance of the input distribution. Hence, add a 5/3 gain.\n",
    "- If we don't have any activations, the standard gaussian is preserved through the layers due to the fan_in scaling factor of the weights."
   ],
   "id": "49b9372c4172e4b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "figure = plt.figure(figsize=(16, 4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.output\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ],
   "id": "9170b2014bf62edf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Gradients of layers",
   "id": "8ed073d8ac05d1ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notes:\n",
    "\n",
    "- We can see that with stable/homogenous activation distributions across the layers, the gradients are all similarly distributed through the layers. This demonstrates that there is no significant vanishing of gradients and the network is learning effectively\n",
    "- If there is significant saturation in the tanh, the gradients have"
   ],
   "id": "d73efd91ba2c658f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "figure = plt.figure(figsize=(16, 4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]):\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.output.grad\n",
    "    print('layer %d (%10s): mean %+.2f, std %f' % (i, layer.__class__.__name__, t.mean(), t.std() ) )\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('gradients')"
   ],
   "id": "882640bce9e1873f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Updates of layers over time",
   "id": "c5cfc99f40dd38f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We want the update to data ratio to be about 1e-3, if under, not learning fast enough, if too high, unstable.\n",
    "\n",
    "- Check LR, Initialisations etc\n",
    "\n"
   ],
   "id": "a85f0790674d01ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(updates[0])",
   "id": "a0c02e50265d6d50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  if p.ndim == 2:\n",
    "    plt.plot([updates[j][i] for j in range(len(updates))])\n",
    "    legends.append('param %d' % i)\n",
    "plt.plot([0, len(updates)], [-3, -3], 'k')\n",
    "plt.title('update ratio, log10')\n",
    "plt.legend(legends);"
   ],
   "id": "c87108ca8cc0a87e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Batch Norm layers can be sprinkled through the NN to maintain stability in the activations by initialising with gaussian",
   "id": "19d2b9bf46ce594b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "57f8b804f3f7fd36",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
