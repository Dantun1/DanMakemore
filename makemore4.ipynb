{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Manual Backprop Exercise",
   "id": "848d09308062a0fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup",
   "id": "927854e81860e6b0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-03T13:57:34.744497Z",
     "start_time": "2026-01-03T13:57:34.739684Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 206
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T13:57:34.802738Z",
     "start_time": "2026-01-03T13:57:34.796419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ],
   "id": "a20cad57acacc166",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "execution_count": 207
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T13:57:34.845366Z",
     "start_time": "2026-01-03T13:57:34.841959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = sorted(set(''.join(words)))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ],
   "id": "88a10b83acecf5fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "execution_count": 208
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T13:57:35.068453Z",
     "start_time": "2026-01-03T13:57:34.887127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []\n",
    "\n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ],
   "id": "bc3b44bf4c17299d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "execution_count": 209
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise",
   "id": "32ff6fd71feb9d65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T13:57:35.104457Z",
     "start_time": "2026-01-03T13:57:35.102517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cmp(s, dt, t):\n",
    "    \"\"\"Utility function to compare my gradient with PyTorch's\"\"\"\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ],
   "id": "3b6eb1e5ab5d03b0",
   "outputs": [],
   "execution_count": 210
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T13:57:35.151041Z",
     "start_time": "2026-01-03T13:57:35.146268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NN setup\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ],
   "id": "9246c8f008a3026b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "execution_count": 211
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T13:57:35.199701Z",
     "start_time": "2026-01-03T13:57:35.197351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ],
   "id": "34de62cce366d369",
   "outputs": [],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T15:21:03.504907Z",
     "start_time": "2026-01-03T15:21:03.496604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ],
   "id": "1c9e88b75559b61c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3367, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 284
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T13:57:35.296644Z",
     "start_time": "2026-01-03T13:57:35.294600Z"
    }
   },
   "cell_type": "code",
   "source": "hprebn.shape",
   "id": "bf72677a3c59ffaa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 214
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "d### Exercise 1: Backprop through whole network manually",
   "id": "4883c5fe8c7e5e57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T15:29:17.200909Z",
     "start_time": "2026-01-03T15:29:17.191227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loss is the average negative log likelihood i.e. -1/n ( log(a) + lob(b)...), so dlogprogs is -1/n for each values a,b... and 0 for the other values\n",
    "# Holds derivative of loss wrt all elements of log probs. The elements not plucked out have 0 derivative while the rest have -1/n\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1/n\n",
    "\n",
    "# This is dlogprobs/dprobs x dloss/dlogprobs. dlogprobs/dprobs is derivative of an elementwise log, so the derivatives are 1/probs as d(log(x)) = 1/x for each p in probs.\n",
    "# We have how much tweaking each Xij in dprobs impacts the loss, so using the chain rule tweaking each element in dprobs has impact of  local derivative (effect on logprobs) x error signal (logprobs effect on loss).\n",
    "dprobs =(1.0/probs) * dlogprobs\n",
    "\n",
    "# Probs = counts * counts_sum_inv. The elementwise derivative is counts * dprobs, but since counts_sum_inv is broadcasted to match the dimensions of counts, we must sum and collapse it into a column vector of the derivatives wrt each element of sum inv. Essentially the actual derivative is the sum of all the items of the matrix row's derivative\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1,keepdim=True)\n",
    "\n",
    "# dcounts_sum is trivial as counts_sum_inv just equals 1/counts_sum so simple power rule application\n",
    "dcounts_sum = -1 * counts_sum **-2 * dcounts_sum_inv\n",
    "\n",
    "# Dcounts has 2 components contributing to it, we need to accumulate the gradients.\n",
    "# First, the gradient directly from counts_sum_inv tweaks\n",
    "dcounts = dprobs * counts_sum_inv\n",
    "# Then since counts_sum is a function of counts. dcounts_sum/dcounts * dloss / dcounts. Tweaking each individual element of counts by 1 tweaks corresponding counts_sum entry by 1, hence tweaking loss by that corresponding entry in dcounts_sum.\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "# Easy backprop through elementwise exp()\n",
    "dnorm_logits = dcounts * norm_logits.exp()\n",
    "# Backprop through simple addition simply passes back the error signal\n",
    "dlogits = dnorm_logits\n",
    "# logit_maxes is a 32x1 tensor, it is broadcasted (to 32x27) + elementwise subtracted from 32x27 logits to get norm logits.\n",
    "# Hence tweaking each element of logit maxes has the cumulative effect of -1 * the sum of each row of dnorm_logits\n",
    "dlogit_maxes = (-dnorm_logits).sum(1,keepdim=True)\n",
    "# Then since dlogit_maxes is a function of logits, we accumulate more gradient into dlogits.\n",
    "# The gradient backpropped is the max components of each row multiplied by dlogit_maxes.\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=vocab_size) * dlogit_maxes\n",
    "# For the matrix operations, the derivative is a matrix multiply between transpose and error signal. Just remember dimensions must make sense. The bias is added to each row so tweaking each bias component changes dL by the sum of dlogits for a column\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "# Simple backprop through elementwise tanh.\n",
    "dhpreact = dh * (1 - h**2)\n",
    "# Batchnorm layer\n",
    "# Similar logic re broadcasting as previously\n",
    "dbngain = (dhpreact * bnraw).sum(0,keepdim = True)\n",
    "dbnbias = dhpreact.sum(0, keepdim = True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnvar_inv = (dbnraw * bndiff).sum(0)\n",
    "dbndiff = dbnraw * bnvar_inv\n",
    "dbnvar = -0.5 * (bnvar + 1e-5)  ** -1.5 * dbnvar_inv\n",
    "dbndiff2 = dbnvar * (1/(n-1)) * torch.ones_like(bndiff2)\n",
    "dbndiff += 2 * bndiff * dbndiff2\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "dhprebn = torch.ones_like(hprebn) * dbndiff\n",
    "dhprebn += torch.ones_like(hprebn) * 1/n * dbnmeani\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "\n",
    "# Finally, backprop the gradients to the embedding vectors.\n",
    "# Emb is a 3d tensor holding, for each triple of indices in Xb, the 10 dim char features for each character\n",
    "# All Demb does is hold the gradients from dembcat in a different format, therefore we just view it in necessary dimension.\n",
    "demb = dembcat.view(-1,3,10)\n",
    "# Now we have the gradients for each feature vector in the batch, we pass back to C to actually adjust the feature vectors\n",
    "# For each character in the demb gradients we have, we need to increment the gradients of the actual feature encoding.\n",
    "\n",
    "# Initialise to 0\n",
    "dC = torch.zeros_like(C)\n",
    "# For each training example\n",
    "for i in range(demb.shape[0]):\n",
    "    # For each feature vector (e.g. 3 per example). Represents the gradients vector/tweak impact per character\n",
    "    for j in range(demb.shape[1]):\n",
    "        # Access the 10 dimensional gradients vector\n",
    "        gradients = demb[i,j]\n",
    "        # Get the index of the character\n",
    "        ix = Xb[i,j]\n",
    "        # Accumulate the gradient into dC\n",
    "        dC[ix] += gradients"
   ],
   "id": "707ecb02a4fe52c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "           2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01],\n",
       "         [-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "           2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01],\n",
       "         [-9.6478e-01, -2.3211e-01, -3.4762e-01,  3.3244e-01, -1.3263e+00,\n",
       "           1.1224e+00,  5.9641e-01,  4.5846e-01,  5.4011e-02, -1.7400e+00]],\n",
       "\n",
       "        [[ 1.2815e+00, -6.3182e-01, -1.2464e+00,  6.8305e-01, -3.9455e-01,\n",
       "           1.4388e-02,  5.7216e-01,  8.6726e-01,  6.3149e-01, -1.2230e+00],\n",
       "         [ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2198e-01,  5.1007e-01,\n",
       "           1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
       "         [-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "           2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01]],\n",
       "\n",
       "        [[-5.6533e-01,  5.4281e-01,  1.7549e-01, -2.2901e+00, -7.0928e-01,\n",
       "          -2.9284e-01, -2.1803e+00,  7.9311e-02,  9.0187e-01,  1.2028e+00],\n",
       "         [ 1.1560e-01,  8.0319e-01,  5.4108e-01, -1.1646e+00,  1.4756e-01,\n",
       "          -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01, -7.8305e-01],\n",
       "         [-1.3257e+00,  1.4670e-01,  1.6913e-01, -1.5397e+00, -7.2759e-01,\n",
       "           1.1491e+00, -8.7462e-01, -2.9771e-01, -1.3707e+00,  1.1500e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "           2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01]],\n",
       "\n",
       "        [[-5.6144e-01, -1.3753e-01, -1.3799e-01, -2.0977e+00, -7.9238e-01,\n",
       "           6.0689e-01, -1.4777e+00, -5.1029e-01,  5.6421e-01,  9.6838e-01],\n",
       "         [ 5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
       "           9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01],\n",
       "         [ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2198e-01,  5.1007e-01,\n",
       "           1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.7039e+00,  7.4204e-01,  9.7370e-01,  3.0028e-01, -2.8971e-01,\n",
       "          -3.1566e-01, -8.7898e-01,  1.0661e-01,  1.8598e+00,  5.5752e-02],\n",
       "         [-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "           2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [-3.1114e-01, -3.0603e-01, -1.7495e+00, -1.6335e+00,  3.8761e-01,\n",
       "           4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00]],\n",
       "\n",
       "        [[-3.1114e-01, -3.0603e-01, -1.7495e+00, -1.6335e+00,  3.8761e-01,\n",
       "           4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00],\n",
       "         [-3.1114e-01, -3.0603e-01, -1.7495e+00, -1.6335e+00,  3.8761e-01,\n",
       "           4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00],\n",
       "         [-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "           2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01]],\n",
       "\n",
       "        [[-1.1641e+00,  1.2473e+00, -2.7061e-01, -1.3635e+00,  1.3066e+00,\n",
       "           3.2307e-01,  1.0358e+00, -8.6249e-01, -1.2575e+00,  9.4180e-01],\n",
       "         [-1.2801e+00,  9.2445e-02,  1.0526e-01, -3.9072e-01,  3.1723e-02,\n",
       "          -5.4753e-01,  8.1827e-01, -8.1628e-01, -3.9243e-01, -7.4521e-01],\n",
       "         [-5.6144e-01, -1.3753e-01, -1.3799e-01, -2.0977e+00, -7.9238e-01,\n",
       "           6.0689e-01, -1.4777e+00, -5.1029e-01,  5.6421e-01,  9.6838e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [-9.4649e-01, -1.5941e-01, -1.9336e-01, -3.7660e-01, -4.9158e-02,\n",
       "           9.3866e-02, -6.4533e-01,  1.2108e+00, -7.8198e-01,  3.8449e-01]],\n",
       "\n",
       "        [[ 1.3283e+00, -1.0921e+00, -8.3952e-01,  1.9098e-01, -7.1750e-01,\n",
       "          -3.8668e-01, -1.2542e+00,  1.2068e+00, -1.7102e+00, -4.7701e-01],\n",
       "         [ 5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
       "           9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01],\n",
       "         [ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2198e-01,  5.1007e-01,\n",
       "           1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01]],\n",
       "\n",
       "        [[-2.1286e-01,  5.0950e-01,  3.2713e-01,  1.9661e+00, -2.4091e-01,\n",
       "          -7.9515e-01,  2.7198e-01, -1.1100e+00, -4.5285e-01, -4.9578e-01],\n",
       "         [-3.1114e-01, -3.0603e-01, -1.7495e+00, -1.6335e+00,  3.8761e-01,\n",
       "           4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00],\n",
       "         [-1.3257e+00,  1.4670e-01,  1.6913e-01, -1.5397e+00, -7.2759e-01,\n",
       "           1.1491e+00, -8.7462e-01, -2.9771e-01, -1.3707e+00,  1.1500e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [-9.6478e-01, -2.3211e-01, -3.4762e-01,  3.3244e-01, -1.3263e+00,\n",
       "           1.1224e+00,  5.9641e-01,  4.5846e-01,  5.4011e-02, -1.7400e+00],\n",
       "         [ 1.1560e-01,  8.0319e-01,  5.4108e-01, -1.1646e+00,  1.4756e-01,\n",
       "          -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01, -7.8305e-01]],\n",
       "\n",
       "        [[ 1.1560e-01,  8.0319e-01,  5.4108e-01, -1.1646e+00,  1.4756e-01,\n",
       "          -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01, -7.8305e-01],\n",
       "         [ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2198e-01,  5.1007e-01,\n",
       "           1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
       "         [-1.3257e+00,  1.4670e-01,  1.6913e-01, -1.5397e+00, -7.2759e-01,\n",
       "           1.1491e+00, -8.7462e-01, -2.9771e-01, -1.3707e+00,  1.1500e-01]],\n",
       "\n",
       "        [[ 1.2815e+00, -6.3182e-01, -1.2464e+00,  6.8305e-01, -3.9455e-01,\n",
       "           1.4388e-02,  5.7216e-01,  8.6726e-01,  6.3149e-01, -1.2230e+00],\n",
       "         [ 1.1560e-01,  8.0319e-01,  5.4108e-01, -1.1646e+00,  1.4756e-01,\n",
       "          -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01, -7.8305e-01],\n",
       "         [ 1.1560e-01,  8.0319e-01,  5.4108e-01, -1.1646e+00,  1.4756e-01,\n",
       "          -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01, -7.8305e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [-9.6478e-01, -2.3211e-01, -3.4762e-01,  3.3244e-01, -1.3263e+00,\n",
       "           1.1224e+00,  5.9641e-01,  4.5846e-01,  5.4011e-02, -1.7400e+00],\n",
       "         [-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "           2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01]],\n",
       "\n",
       "        [[-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "           2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01],\n",
       "         [ 1.2815e+00, -6.3182e-01, -1.2464e+00,  6.8305e-01, -3.9455e-01,\n",
       "           1.4388e-02,  5.7216e-01,  8.6726e-01,  6.3149e-01, -1.2230e+00],\n",
       "         [-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "           2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.1560e-01,  8.0319e-01,  5.4108e-01, -1.1646e+00,  1.4756e-01,\n",
       "          -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01, -7.8305e-01],\n",
       "         [-5.6144e-01, -1.3753e-01, -1.3799e-01, -2.0977e+00, -7.9238e-01,\n",
       "           6.0689e-01, -1.4777e+00, -5.1029e-01,  5.6421e-01,  9.6838e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [-1.0188e+00, -8.3777e-01, -2.1057e+00, -2.6044e-01, -1.7149e+00,\n",
       "          -3.3787e-01, -1.8263e+00, -8.3897e-01, -1.5723e+00,  4.5795e-01],\n",
       "         [-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "           2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01]],\n",
       "\n",
       "        [[-1.3257e+00,  1.4670e-01,  1.6913e-01, -1.5397e+00, -7.2759e-01,\n",
       "           1.1491e+00, -8.7462e-01, -2.9771e-01, -1.3707e+00,  1.1500e-01],\n",
       "         [ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2198e-01,  5.1007e-01,\n",
       "           1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01],\n",
       "         [-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "           2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.2815e+00, -6.3182e-01, -1.2464e+00,  6.8305e-01, -3.9455e-01,\n",
       "           1.4388e-02,  5.7216e-01,  8.6726e-01,  6.3149e-01, -1.2230e+00]],\n",
       "\n",
       "        [[ 1.2648e+00,  1.4625e+00,  1.1199e+00,  9.9539e-01, -1.2353e+00,\n",
       "           7.3818e-01,  8.1415e-01, -7.3806e-01,  5.6714e-01, -1.4601e+00],\n",
       "         [ 1.1560e-01,  8.0319e-01,  5.4108e-01, -1.1646e+00,  1.4756e-01,\n",
       "          -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01, -7.8305e-01],\n",
       "         [-4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "           2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [-5.6533e-01,  5.4281e-01,  1.7549e-01, -2.2901e+00, -7.0928e-01,\n",
       "          -2.9284e-01, -2.1803e+00,  7.9311e-02,  9.0187e-01,  1.2028e+00],\n",
       "         [ 5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
       "           9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 6.1690e-01,  1.5160e+00, -1.0447e+00, -6.6414e-01, -7.2390e-01,\n",
       "           1.7507e+00,  1.7530e-01,  9.9280e-01, -6.2787e-01,  7.7023e-02]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.2815e+00, -6.3182e-01, -1.2464e+00,  6.8305e-01, -3.9455e-01,\n",
       "           1.4388e-02,  5.7216e-01,  8.6726e-01,  6.3149e-01, -1.2230e+00],\n",
       "         [ 1.1560e-01,  8.0319e-01,  5.4108e-01, -1.1646e+00,  1.4756e-01,\n",
       "          -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01, -7.8305e-01]],\n",
       "\n",
       "        [[-9.4649e-01, -1.5941e-01, -1.9336e-01, -3.7660e-01, -4.9158e-02,\n",
       "           9.3866e-02, -6.4533e-01,  1.2108e+00, -7.8198e-01,  3.8449e-01],\n",
       "         [ 1.1560e-01,  8.0319e-01,  5.4108e-01, -1.1646e+00,  1.4756e-01,\n",
       "          -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01, -7.8305e-01],\n",
       "         [ 1.2815e+00, -6.3182e-01, -1.2464e+00,  6.8305e-01, -3.9455e-01,\n",
       "           1.4388e-02,  5.7216e-01,  8.6726e-01,  6.3149e-01, -1.2230e+00]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2198e-01,  5.1007e-01,\n",
       "           1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01]],\n",
       "\n",
       "        [[-1.0725e+00,  7.2762e-01,  5.1114e-02,  1.3095e+00, -8.0220e-01,\n",
       "          -8.5042e-01, -1.8068e+00,  1.2523e+00, -1.2256e+00,  1.2165e+00],\n",
       "         [ 1.1560e-01,  8.0319e-01,  5.4108e-01, -1.1646e+00,  1.4756e-01,\n",
       "          -1.0006e+00,  3.8012e-01,  4.7328e-01, -9.1027e-01, -7.8305e-01],\n",
       "         [ 4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2198e-01,  5.1007e-01,\n",
       "           1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01]],\n",
       "\n",
       "        [[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "          -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01],\n",
       "         [ 1.2815e+00, -6.3182e-01, -1.2464e+00,  6.8305e-01, -3.9455e-01,\n",
       "           1.4388e-02,  5.7216e-01,  8.6726e-01,  6.3149e-01, -1.2230e+00],\n",
       "         [ 5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
       "           9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01]]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 291
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T15:10:47.671509Z",
     "start_time": "2026-01-03T15:10:47.664894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ],
   "id": "b2b426ebc1c04fb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "execution_count": 275
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 2: Softmax + CrossEntropy simpler backprop analytically\n",
    "\n",
    "Done pen/paper"
   ],
   "id": "c8f3134ea0f0a968"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:03:15.180481Z",
     "start_time": "2026-01-05T10:03:15.149071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ],
   "id": "9c38420382e79c1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3366892337799072 diff: -2.384185791015625e-07\n"
     ]
    }
   ],
   "execution_count": 292
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:27:55.258977Z",
     "start_time": "2026-01-05T10:27:55.252863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits)"
   ],
   "id": "499774a701653cc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 4.423782229423523e-09\n"
     ]
    }
   ],
   "execution_count": 294
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since the gradients of the logits per row are just the softmax probs, and p-1 for the correct value, think of each row in logits as being adjusted like a pulley system where the total adjustment is 1 as p1 + p2 + p3 + (py-1) +.. +pn = 0. We are adjusting the logits to make the likely outcome more likely, exactly by the amount we are reducing the rest",
   "id": "59d7c6823a347ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 3: BatchNorm layer simpler backprop analytically\n",
    "\n",
    "Done pen/paper"
   ],
   "id": "82bab4db33b0b82b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:34:53.046712Z",
     "start_time": "2026-01-05T10:34:53.040778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ],
   "id": "ac312e29525cd99b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "execution_count": 297
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T10:34:53.340265Z",
     "start_time": "2026-01-05T10:34:53.336593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ],
   "id": "4f7eb04b49e38186",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "execution_count": 298
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cd23fdc80efc47d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
